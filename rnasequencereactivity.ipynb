{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import gc\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "from hyperparameters import config\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNA_Dataset(Dataset):\n",
    "    def __init__(self, df,df_457=None,df_307=None,mode = 'train'):\n",
    "        self.mode = mode\n",
    "        df_a = df.loc[df.experiment_type == '2A3_MaP'].reset_index(drop=True)\n",
    "        df_d = df.loc[df.experiment_type == 'DMS_MaP'].reset_index(drop=True)\n",
    "        self.seq = df_a['sequence'].values\n",
    "        self.react_a = df_a[[c for c in df_a.columns if 'reactivity_0' in c]].values\n",
    "        self.react_d = df_d[[c for c in df_d.columns if 'reactivity_0' in c]].values\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            a_457 = df_457.loc[df_457.experiment_type == '2A3_MaP'].reset_index(drop=True)\n",
    "            d_457 = df_457.loc[df_457.experiment_type == 'DMS_MaP'].reset_index(drop=True)\n",
    "            self.seq_457 = a_457['sequence'].values\n",
    "            self.reactivity_a_457 = a_457[[c for c in a_457.columns if 'reactivity' in c]].values\n",
    "            self.reactivity_d_457 = d_457[[c for c in d_457.columns if 'reactivity' in c]].values\n",
    "        \n",
    "        \n",
    "            a_307 = df_307.loc[df_307.experiment_type == '2A3_MaP'].reset_index(drop=True)\n",
    "            d_307 = df_307.loc[df_307.experiment_type == 'DMS_MaP'].reset_index(drop=True)\n",
    "            self.seq_307 = a_307['sequence'].values\n",
    "            self.reactivity_a_307 = a_307[[c for c in a_307.columns if 'reactivity' in c]].values\n",
    "            self.reactivity_d_307 = d_307[[c for c in d_307.columns if 'reactivity' in c]].values\n",
    "            del df_a, df_d,a_457,d_457,a_307,d_307\n",
    "            gc.collect()\n",
    "            \n",
    "        self.nucleotid_mapper = {'A':0,'G':1,'C':2,'U':3}\n",
    "            \n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seq)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            idx = np.random.choice(3000)\n",
    "            if np.random.choice(10) == 9:\n",
    "                # using fake sequences\n",
    "                if np.random.choice(10) >=5:\n",
    "                    seq = self.seq_457[idx]\n",
    "                    target = torch.from_numpy(np.stack([self.reactivity_a_457[idx], self.reactivity_d_457[idx]], -1))\n",
    "                    emb = torch.LongTensor([self.nucleotid_mapper[x] for x in seq])\n",
    "                    return emb,target\n",
    "                else:\n",
    "                    seq = self.seq_307[idx]\n",
    "                    target = torch.from_numpy(np.stack([self.reactivity_a_307[idx], self.reactivity_d_307[idx]], -1))\n",
    "                    emb = torch.LongTensor([self.nucleotid_mapper[x] for x in seq])\n",
    "                    return emb,target\n",
    "                \n",
    "            seq= self.seq[item]\n",
    "            emb = torch.LongTensor([self.nucleotid_mapper[x] for x in seq])\n",
    "            target = torch.from_numpy(np.stack([self.react_a[item], self.react_d[item]], -1))\n",
    "\n",
    "\n",
    "            return emb,target\n",
    "        else:\n",
    "            seq= self.seq[item]\n",
    "            emb = torch.LongTensor([self.nucleotid_mapper[x] for x in seq])\n",
    "            target = torch.from_numpy(np.stack([self.react_a[item], self.react_d[item]], -1))\n",
    "\n",
    "\n",
    "            return emb,target\n",
    "        \n",
    "class RNA_Test_Dataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.seq = df['sequence'].values\n",
    "        self.nucleotid_mapper = {'A': 0, 'G': 1, 'C': 2, 'U': 3}\n",
    "        \n",
    "        del df\n",
    "        gc.collect()    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seq)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        seq= self.seq[item]\n",
    "        emb = torch.LongTensor([self.nucleotid_mapper[x] for x in seq])\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet('train_files/clean_train.parquet')\n",
    "train_307 = pd.read_parquet('train_files/generated_seq_307.parquet')\n",
    "train_457 = pd.read_parquet('train_files/generated_seq_457_new.parquet')\n",
    "valid = pd.read_parquet('train_files/valid_with_structure.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, dim=192, depth=12, head_size=32, **kwargs):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(4,dim)\n",
    "        self.pos_encoder = PositionalEncoding(dim)\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=dim, nhead=dim//head_size, dim_feedforward=4*dim,\n",
    "                dropout=0.5, activation='relu', batch_first=True, norm_first=True), depth)\n",
    "        self.linear = nn.Linear(dim,2)\n",
    "        self.dim_model = dim\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # pos = torch.arange(x.shape[1], device=x.device).unsqueeze(0)\n",
    "        # pos = self.pos_encoder(pos)\n",
    "        x = self.emb(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        \n",
    "        x = self.transformer(x)\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim=16, M=10000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.M = M\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(self.M) / half_dim\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * (-emb))\n",
    "        emb = x[...,None] * emb[None,...]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 500):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "class AbsolutePositionalEncoder(nn.Module):\n",
    "    def __init__(self, emb_dim, max_position=512):\n",
    "        super(AbsolutePositionalEncoder, self).__init__()\n",
    "        self.position = torch.arange(max_position).unsqueeze(1)\n",
    "\n",
    "        self.positional_encoding = torch.zeros(1, max_position, emb_dim)\n",
    "\n",
    "        _2i = torch.arange(0, emb_dim, step=2).float()\n",
    "\n",
    "        # PE(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
    "        self.positional_encoding[0, :, 0::2] = torch.sin(self.position / (10000 ** (_2i / emb_dim)))\n",
    "\n",
    "        # PE(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
    "        self.positional_encoding[0, :, 1::2] = torch.cos(self.position / (10000 ** (_2i / emb_dim)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # batch_size, input_len, embedding_dim\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        return self.positional_encoding[:batch_size, :seq_len, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = RNA_Dataset(train,train_457,train_307,mode='train')\n",
    "valid_ds = RNA_Dataset(valid,mode='valid')\n",
    "train_loader = DataLoader(train_ds,batch_size=config.batch_size,shuffle=True,num_workers=8,drop_last=True)\n",
    "valid_loader = DataLoader(valid_ds,batch_size=config.batch_size,shuffle=False,num_workers=8,drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Inferenece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(pred, target):\n",
    "    if target.shape[0] > 206:\n",
    "        loss = F.l1_loss(pred, target.clip(0,1), reduction='none')\n",
    "        loss = 0.5 * loss[~torch.isnan(loss)].mean()\n",
    "    else:\n",
    "        loss = F.l1_loss(pred, target.clip(0,1), reduction='none')\n",
    "        loss = loss[~torch.isnan(loss)].mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(model,train_loader,valid_loader,loss_fn,resume = False):\n",
    "    optimizer = torch.optim.SGD(model.parameters(),lr=0.005,momentum=0.9,weight_decay=0.0005)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,verbose=True,patience=5)\n",
    "    last_epoch = 0\n",
    "    best_train_loss =float('inf')\n",
    "    best_valid_loss = float('inf')\n",
    "    best_compet_score = float('inf')\n",
    "    model.to(device)\n",
    "    if resume:\n",
    "        print('Loading last checkpoint...')\n",
    "        model.load_state_dict(config.ckp['weights'])\n",
    "        optimizer.load_state_dict(config.ckp['optimizer'])\n",
    "        best_train_loss = config.ckp['best_train_loss']\n",
    "        best_valid_loss = config.ckp['best_valid_loss']\n",
    "        best_compet_score = config.ckp['best_compet_score']\n",
    "        last_epoch = config.ckp['epoch']\n",
    "    print(f'Starting train from: Epoch: {last_epoch} | Best valid loss : {best_valid_loss:.5f} | compet score: {best_compet_score:.5f}\\t')\n",
    "\n",
    "    for epoch in range(last_epoch,1000):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "\n",
    "        for x,y in tqdm(train_loader):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            x,y = x.to(device),y.to(device).squeeze(dim=0)\n",
    "            prediction = model(x)\n",
    "            pred_a,pred_d = prediction.squeeze(dim=0)[:,0].unsqueeze(dim=-1),prediction.squeeze(dim=0)[:,1].unsqueeze(dim=-1)\n",
    "            if prediction.shape[1]<206:\n",
    "                pred_a = F.pad(pred_a,[0,0,0,206-pred_a.shape[0]])\n",
    "                pred_d = F.pad(pred_d,[0,0,0,206-pred_d.shape[0]])\n",
    "            target_a,target_d = y[:,0].unsqueeze(dim=-1),y[:,1].unsqueeze(dim=-1)\n",
    "\n",
    "\n",
    "            loss = (loss_fn(pred_a,target_a) + loss_fn(pred_d,target_d))/2\n",
    "            loss.sum().backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.sum().item()\n",
    "\n",
    "\n",
    "\n",
    "        train_loss = train_loss/len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        compet_score = 0\n",
    "        with ((torch.inference_mode())):\n",
    "            for x,y in tqdm(valid_loader):\n",
    "                x,y = x.to(device),y.to(device).squeeze(dim=0)\n",
    "                prediction = model(x)\n",
    "                \n",
    "                pred_a,pred_d = prediction.squeeze(dim=0)[:,0].unsqueeze(dim=-1),prediction.squeeze(dim=0)[:,1].unsqueeze(dim=-1)\n",
    "                if prediction.shape[1]<206:\n",
    "                    pred_a = F.pad(pred_a,[0,0,0,206-pred_a.shape[0]])\n",
    "                    pred_d = F.pad(pred_d,[0,0,0,206-pred_d.shape[0]])\n",
    "                    \n",
    "                target_a,target_d = y[:,0].unsqueeze(dim=-1),y[:,1].unsqueeze(dim=-1)\n",
    "                \n",
    "                loss = (loss_fn(pred_a, target_a) + loss_fn(pred_d, target_d)) / 2\n",
    "                \n",
    "\n",
    "                valid_loss += loss.sum().item()\n",
    "\n",
    "                score = (loss_fn(pred_a, target_a) + loss_fn(pred_d, target_d)) / 2\n",
    "                compet_score += score.item()\n",
    "            compet_score /= len(valid_loader)\n",
    "            valid_loss = valid_loss/len(valid_loader)\n",
    "\n",
    "        scheduler.step(compet_score)\n",
    "        print(f\"Epoch {epoch + 1}:| Train Loss: {train_loss:.5f} | Valid Loss: {valid_loss:.5f} | compet score: {compet_score:.5f}\")\n",
    "        # saving model\n",
    "        if valid_loss < best_valid_loss or compet_score < best_compet_score:\n",
    "            if train_loss < best_train_loss:\n",
    "                best_train_loss = train_loss\n",
    "            if valid_loss < best_valid_loss:\n",
    "                best_valid_loss = valid_loss\n",
    "            if compet_score < best_compet_score:\n",
    "                best_compet_score = compet_score\n",
    "\n",
    "            torch.save({'weights': model.state_dict(),\n",
    "                            'optimizer': optimizer.state_dict(),\n",
    "                            'epoch': epoch+1,\n",
    "                            'best_train_loss': best_train_loss,\n",
    "                            'best_valid_loss': best_valid_loss,\n",
    "                            'best_compet_score': best_compet_score,\n",
    "                            },f'logs/train_logs.pth')\n",
    "            print('Train logs saved.')\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        if (epoch+1) % 16 == 0:\n",
    "            print('Creating submission...\\t')\n",
    "\n",
    "            test_seq = pd.read_parquet('train_files/test_seq_struct.parquet')\n",
    "            test_ds = RNA_Test_Dataset(test_seq)\n",
    "            test_dataloader = DataLoader(test_ds, batch_size=1, drop_last=False, num_workers=8)\n",
    "\n",
    "            model.eval()\n",
    "            preds = []\n",
    "            with torch.inference_mode():\n",
    "                for x in tqdm(test_dataloader):\n",
    "                    prediction = model(x.to(device))\n",
    "\n",
    "                    preds.append(prediction.squeeze(dim=0).detach().cpu().numpy())\n",
    "\n",
    "            preds = np.concatenate(preds, dtype=np.float32)\n",
    "\n",
    "            submission = pd.DataFrame({'reactivity_DMS_MaP': preds[:, 1], 'reactivity_2A3_MaP': preds[:, 0]})\n",
    "            submission = submission.clip(0, 1, axis=0)\n",
    "            submission = submission.reset_index().rename(columns={'index': 'id'})\n",
    "            submission.to_parquet(f'logs/submission_{epoch+1}_valid_metric:{compet_score:.5f}_valid_loss:{valid_loss:.5f}.parquet')\n",
    "            del test_seq, test_ds,test_dataloader,submission,preds\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerModel(dim=512,depth=12).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting train from: Epoch: 0 | Best valid loss : inf | compet score: inf\t\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a5f3406c7354d65b5d88be26d4bd8c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/164782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caee49777c944a97a35265f079b2d628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16479 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:| Train Loss: 0.23578 | Valid Loss: 0.23480 | compet score: 0.23480\n",
      "Train logs saved.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c5149d2fd7b4512ab2b7f9ac69157d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/164782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76c2b25758554247bf5d1f8f61790ad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16479 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:| Train Loss: 0.22501 | Valid Loss: 0.23481 | compet score: 0.23481\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "442269b15dc747fabd9ceca00b62937e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/164782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/cerenov/Projects/ribonanzaRNA/rnasequencereactivity.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/cerenov/Projects/ribonanzaRNA/rnasequencereactivity.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m learn(model,train_loader,valid_loader,loss_fn,resume\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "\u001b[1;32m/home/cerenov/Projects/ribonanzaRNA/rnasequencereactivity.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cerenov/Projects/ribonanzaRNA/rnasequencereactivity.ipynb#X15sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cerenov/Projects/ribonanzaRNA/rnasequencereactivity.ipynb#X15sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m x,y \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(device),y\u001b[39m.\u001b[39mto(device)\u001b[39m.\u001b[39msqueeze(dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/cerenov/Projects/ribonanzaRNA/rnasequencereactivity.ipynb#X15sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m prediction \u001b[39m=\u001b[39m model(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cerenov/Projects/ribonanzaRNA/rnasequencereactivity.ipynb#X15sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m pred_a,pred_d \u001b[39m=\u001b[39m prediction\u001b[39m.\u001b[39msqueeze(dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)[:,\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39munsqueeze(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m),prediction\u001b[39m.\u001b[39msqueeze(dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)[:,\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39munsqueeze(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cerenov/Projects/ribonanzaRNA/rnasequencereactivity.ipynb#X15sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mif\u001b[39;00m prediction\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\u001b[39m<\u001b[39m\u001b[39m206\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/cerenov/Projects/ribonanzaRNA/rnasequencereactivity.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cerenov/Projects/ribonanzaRNA/rnasequencereactivity.ipynb#X15sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39memb(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cerenov/Projects/ribonanzaRNA/rnasequencereactivity.ipynb#X15sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_encoder(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/cerenov/Projects/ribonanzaRNA/rnasequencereactivity.ipynb#X15sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cerenov/Projects/ribonanzaRNA/rnasequencereactivity.ipynb#X15sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cerenov/Projects/ribonanzaRNA/rnasequencereactivity.ipynb#X15sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:315\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    312\u001b[0m is_causal \u001b[39m=\u001b[39m make_causal\n\u001b[1;32m    314\u001b[0m \u001b[39mfor\u001b[39;00m mod \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m--> 315\u001b[0m     output \u001b[39m=\u001b[39m mod(output, src_mask\u001b[39m=\u001b[39;49mmask, is_causal\u001b[39m=\u001b[39;49mis_causal, src_key_padding_mask\u001b[39m=\u001b[39;49msrc_key_padding_mask_for_layers)\n\u001b[1;32m    317\u001b[0m \u001b[39mif\u001b[39;00m convert_to_nested:\n\u001b[1;32m    318\u001b[0m     output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mto_padded_tensor(\u001b[39m0.\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:588\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    586\u001b[0m x \u001b[39m=\u001b[39m src\n\u001b[1;32m    587\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm_first:\n\u001b[0;32m--> 588\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sa_block(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm1(x), src_mask, src_key_padding_mask, is_causal\u001b[39m=\u001b[39;49mis_causal)\n\u001b[1;32m    589\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x))\n\u001b[1;32m    590\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:599\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sa_block\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor,\n\u001b[1;32m    598\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 599\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(x, x, x,\n\u001b[1;32m    600\u001b[0m                        attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[1;32m    601\u001b[0m                        key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[1;32m    602\u001b[0m                        need_weights\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, is_causal\u001b[39m=\u001b[39;49mis_causal)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    603\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout1(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.10/site-packages/torch/nn/modules/activation.py:1205\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1191\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1192\u001b[0m         query, key, value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\n\u001b[1;32m   1193\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1202\u001b[0m         average_attn_weights\u001b[39m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1203\u001b[0m         is_causal\u001b[39m=\u001b[39mis_causal)\n\u001b[1;32m   1204\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1205\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mmulti_head_attention_forward(\n\u001b[1;32m   1206\u001b[0m         query, key, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[1;32m   1207\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_bias,\n\u001b[1;32m   1208\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_k, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_v, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_zero_attn,\n\u001b[1;32m   1209\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m   1210\u001b[0m         training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[1;32m   1211\u001b[0m         key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[1;32m   1212\u001b[0m         need_weights\u001b[39m=\u001b[39;49mneed_weights,\n\u001b[1;32m   1213\u001b[0m         attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[1;32m   1214\u001b[0m         average_attn_weights\u001b[39m=\u001b[39;49maverage_attn_weights,\n\u001b[1;32m   1215\u001b[0m         is_causal\u001b[39m=\u001b[39;49mis_causal)\n\u001b[1;32m   1216\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39mand\u001b[39;00m is_batched:\n\u001b[1;32m   1217\u001b[0m     \u001b[39mreturn\u001b[39;00m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/anaconda3/envs/torchenv/lib/python3.10/site-packages/torch/nn/functional.py:5373\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5370\u001b[0m k \u001b[39m=\u001b[39m k\u001b[39m.\u001b[39mview(bsz, num_heads, src_len, head_dim)\n\u001b[1;32m   5371\u001b[0m v \u001b[39m=\u001b[39m v\u001b[39m.\u001b[39mview(bsz, num_heads, src_len, head_dim)\n\u001b[0;32m-> 5373\u001b[0m attn_output \u001b[39m=\u001b[39m scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n\u001b[1;32m   5374\u001b[0m attn_output \u001b[39m=\u001b[39m attn_output\u001b[39m.\u001b[39mpermute(\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(bsz \u001b[39m*\u001b[39m tgt_len, embed_dim)\n\u001b[1;32m   5376\u001b[0m attn_output \u001b[39m=\u001b[39m linear(attn_output, out_proj_weight, out_proj_bias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learn(model,train_loader,valid_loader,loss_fn,resume=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
